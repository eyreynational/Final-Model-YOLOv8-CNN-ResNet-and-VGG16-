{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkQhdX6abQMr9dc/m3pqxb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyreynational/Final-Model-YOLOv8-CNN-ResNet-and-VGG16-/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rbv3_gma4RE0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import os\n",
        "# Tensorflow Libraries\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "# System libraries\n",
        "from pathlib import Path\n",
        "import os.path\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# Visualization Libraries\n",
        "import matplotlib.cm as cm\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "from cycler import cycler\n",
        "import textwrap\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools"
      ],
      "metadata": {
        "id": "lPmIq-QR6WVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "train_dir = '/content/drive/MyDrive/Master/Dataset/trian'  # Update with your path\n",
        "test_dir = '/content/drive/MyDrive/Master/Dataset/test'"
      ],
      "metadata": {
        "id": "wWKRqcN_Nyau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the shape (dimensions) of the training directory\n",
        "train_dir_shape = os.path.abspath(train_dir)\n",
        "print(train_dir_shape)\n",
        "\n",
        "# If you want the size of training dir\n",
        "train_dir_size = os.path.getsize(train_dir)\n",
        "print(train_dir_size)"
      ],
      "metadata": {
        "id": "7hVzaF8xT8i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define a function to count images in each category\n",
        "def count_images_per_category(directory):\n",
        "    category_counts = {}\n",
        "    for category in os.listdir(directory):\n",
        "        category_path = os.path.join(directory, category)\n",
        "        if os.path.isdir(category_path):  # Check if it's a directory\n",
        "            category_counts[category] = len(os.listdir(category_path))\n",
        "    return category_counts\n",
        "\n",
        "# Count images in training and testing sets\n",
        "train_counts = count_images_per_category(train_dir)\n",
        "test_counts = count_images_per_category(test_dir)\n",
        "\n",
        "# Create a bar chart\n",
        "categories = list(train_counts.keys())  # Get category names\n",
        "train_values = list(train_counts.values())  # Get image counts for training set\n",
        "test_values = list(test_counts.values())  # Get image counts for testing set\n",
        "\n",
        "x = np.arange(len(categories))  # the label locations\n",
        "width = 0.35  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width/2, train_values, width, label='Train')\n",
        "rects2 = ax.bar(x + width/2, test_values, width, label='Test')\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Number of Images')\n",
        "ax.set_title('Dataset Distribution by Category')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(categories, rotation=45, ha='right')  # Rotate x-axis labels if needed\n",
        "ax.legend()\n",
        "\n",
        "# Add value labels to the bars\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VtQZ6KSBT6nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (256, 256)"
      ],
      "metadata": {
        "id": "jB3anmoLs67K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load images and their labels from a directory\n",
        "def load_images_from_dir(directory, img_size):\n",
        "    images = []\n",
        "    labels = []\n",
        "    # List only subdirectories (each corresponding to a category)\n",
        "    categories = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
        "    for category in categories:\n",
        "        category_path = os.path.join(directory, category)\n",
        "        for img_name in os.listdir(category_path):\n",
        "            img_path = os.path.join(category_path, img_name)\n",
        "            # Read the image using cv2 (in BGR format)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                # Resize to the common size (256x256)\n",
        "                img = cv2.resize(img, img_size)\n",
        "                images.append(img)\n",
        "                labels.append(category)\n",
        "    return np.array(images), np.array(labels)"
      ],
      "metadata": {
        "id": "SGKgs6-ssSk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IXzRdYLds4hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training images and labels\n",
        "X_train, y_train = load_images_from_dir(train_dir, img_size)\n",
        "print(\"Training data shape:\", X_train.shape)"
      ],
      "metadata": {
        "id": "ywOb6M7bsxPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten each image into a 1D vector\n",
        "n_samples = X_train.shape[0]\n",
        "X_flat = X_train.reshape(n_samples, -1)\n",
        "print(\"Flattened image shape:\", X_flat.shape)"
      ],
      "metadata": {
        "id": "h_dcF0cLtRkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the pixel values (mean=0, std=1)\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_flat.astype(np.float64))"
      ],
      "metadata": {
        "id": "rePKg_VttWHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA to retain 95% of the variance\n",
        "from sklearn.decomposition import PCA  # Import PCA here\n",
        "pca = PCA(n_components=0.95, svd_solver='full')\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "print(\"PCA reduced shape:\", X_pca.shape)"
      ],
      "metadata": {
        "id": "IF6NAFSztc3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = load_images_from_dir(test_dir, img_size)\n",
        "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "X_test_scaled = scaler.transform(X_test_flat.astype(np.float64))\n",
        "X_test_pca = pca.transform(X_test_scaled)\n"
      ],
      "metadata": {
        "id": "I0Qgvvt-9I2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the cumulative explained variance ratio to see how many components were selected\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9Jts3j7uY5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data Augmentation for the Training Set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,                # Normalization\n",
        "    rotation_range=20,             # Random rotations\n",
        "    width_shift_range=0.2,         # Horizontal shifts\n",
        "    height_shift_range=0.2,        # Vertical shifts\n",
        "    shear_range=0.2,               # Shear transformations\n",
        "    zoom_range=0.2,                # Random zoom\n",
        "    horizontal_flip=True,          # Flip horizontally\n",
        "    fill_mode='nearest'            # Fill mode for shifts\n",
        ")\n",
        "\n",
        "# For the Test Set, only rescale\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load the training set\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # For multi-class classification\n",
        ")\n",
        "\n",
        "# Load the test set\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Check Class Balance\n",
        "print(\"Class distribution in training set:\", Counter(train_generator.classes))\n",
        "print(\"Class distribution in test set:\", Counter(test_generator.classes))"
      ],
      "metadata": {
        "id": "VMIH-clRUsm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_generator.class_indices)\n",
        "print(test_generator.class_indices)"
      ],
      "metadata": {
        "id": "ve7XEjZ2jL2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ... (your previous code for ImageDataGenerator and generators) ...\n",
        "\n",
        "# Function to display images with labels\n",
        "def show_images_with_labels(generator, num_images=5):\n",
        "    \"\"\"Displays a specified number of images with their labels.\"\"\"\n",
        "    images, labels = next(generator)  # Get a batch of images and labels\n",
        "    num_images = min(num_images, images.shape[0])  # Limit to available images\n",
        "\n",
        "    plt.figure(figsize=(15, 5))  # Adjust figure size as needed\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.title(f\"Label: {np.argmax(labels[i])}\")  # Display numerical label\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "show_images_with_labels(train_generator)  # Display images from the training set"
      ],
      "metadata": {
        "id": "2KRoNwZZjNY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation for the Training Set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,                # Normalization\n",
        "    rotation_range=40,             # Random rotations\n",
        "    width_shift_range=0.2,         # Horizontal shifts\n",
        "    height_shift_range=0.2,        # Vertical shifts\n",
        "    shear_range=0.2,               # Shear transformations\n",
        "    zoom_range=0.2,                # Random zoom\n",
        "    horizontal_flip=True,          # Flip horizontally\n",
        "    fill_mode='nearest'            # Fill mode for shifts\n",
        ")"
      ],
      "metadata": {
        "id": "nfpaqqRAOAPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the Test Set, only rescale\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load the training set\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # For multi-class classification\n",
        ")"
      ],
      "metadata": {
        "id": "DK8Xs78WODUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test set\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "4lvvW95nOJTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Architecture\n",
        "model = models.Sequential()\n",
        "\n",
        "# Add Convolutional layers\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n"
      ],
      "metadata": {
        "id": "Eibcdz1_OTYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten and add Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))  # Dropout to prevent overfitting\n",
        "model.add(layers.Dense(5, activation='softmax'))  # Output layer for 5 classes"
      ],
      "metadata": {
        "id": "P2fbukcvOXml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',  # You can try other optimizers like RMSprop, SGD\n",
        "    loss='categorical_crossentropy',  # Multi-class classification loss\n",
        "    metrics=['accuracy']  # Track accuracy during training\n",
        ")\n"
      ],
      "metadata": {
        "id": "2ThjLCfvOZAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=70,  # Adjust the number of epochs based on overfitting/underfitting\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=test_generator.samples // test_generator.batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "kZ3cttAdOcuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=2)\n",
        "print(f\"Test Accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "BxcfRsCdOnld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "train_loss, train_acc = model.evaluate(train_generator, verbose=2)\n",
        "print(f\"Train Accuracy: {train_acc}\")"
      ],
      "metadata": {
        "id": "U_Cg4MUXYtHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "Y_pred = model.predict(test_generator)\n",
        "y_pred = np.argmax(Y_pred, axis=1)"
      ],
      "metadata": {
        "id": "bQbo2qe0nPKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report for precision, recall, f1-score\n",
        "print('Classification Report')\n",
        "target_names = ['Powdery_mildew', 'Bacterial_spot', 'Late_blight', 'Healthy', 'Mosaic_virus']\n",
        "print(classification_report(test_generator.classes, y_pred, target_names=target_names))\n"
      ],
      "metadata": {
        "id": "7l9DoLOcnxHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print('Classification Report')\n",
        "target_names = ['Powdery_mildew', 'Bacterial_spot', 'Late_blight', 'Healthy', 'Mosaic_virus']\n",
        "print(classification_report(test_generator.classes, y_pred, target_names=target_names, digits=4))\n"
      ],
      "metadata": {
        "id": "RF8CuArguboy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get the confusion matrix\n",
        "cm = confusion_matrix(test_generator.classes, y_pred)\n",
        "\n",
        "# Class labels\n",
        "class_names = ['Powdery_mildew', 'Bacterial_spot', 'Late_blight', 'Healthy', 'Mosaic_virus']\n",
        "\n",
        "# Calculate per-class accuracy\n",
        "print(\"Per-Class Accuracy:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    correct = cm[i, i]\n",
        "    total = cm[i].sum()\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    print(f\"{class_name}: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Rkq_RgPmIBBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix for detailed error analysis\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred))"
      ],
      "metadata": {
        "id": "Alce1OZcn0xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the confusion matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix # Import confusion_matrix\n",
        "\n",
        "# Get predictions for the test set\n",
        "y_pred = model.predict(test_generator).argmax(axis=1)  # Get predicted class labels\n",
        "\n",
        "conf_matrix = confusion_matrix(test_generator.classes, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=test_generator.class_indices,\n",
        "            yticklabels=test_generator.class_indices)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wATXhrWNY1ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model to a file\n",
        "model.save('https://drive.google.com/drive/folders/1tmOAHKL5YK_dxG9ufIbU_DDsIhSPcZZH/CNN_model.h5')"
      ],
      "metadata": {
        "id": "50XJ4ARR5TUo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}